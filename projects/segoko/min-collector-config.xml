<?xml version="1.0" encoding="UTF-8"?>
<!-- 
Copyright 2010-2013 Norconex Inc.

This file is part of Norconex HTTP Collector.

Norconex HTTP Collector is free software: you can redistribute it and/or modify
it under the terms of the GNU General Public License as published by
the Free Software Foundation, either version 3 of the License, or
(at your option) any later version.

Norconex HTTP Collector is distributed in the hope that it will be useful, 
but WITHOUT ANY WARRANTY; without even the implied warranty of 
MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
GNU General Public License for more details.

You should have received a copy of the GNU General Public License
along with Norconex HTTP Collector. If not, see <http://www.gnu.org/licenses/>.
-->

<!-- This self-documented configuration file is meant to be used as a reference
     or starting point for a new configuration. 
     It contains all core features offered in this release.  Sometimes
     multiple implementations are available for a given feature. Refer 
     to site documentation for more options and complete description of 
     each features.
     -->
<httpcollector id="httpcollector-segoko">

  <!-- Variables: Optionally define variables in this configuration file
       using the "set" directive, or by using a file of the same name
       but with the extension ".variables" or ".properties".  Refer 
       to site documentation to find out what each extension does.
       Finally, once can pass an optional properties file when starting the
       crawler.  The following is good practice to reference frequently 
       used classes in a shorter way.
       -->
  #set($workdir = "work")
  #set($basepkg = "com.norconex.collector.http")
  #set($extension = "${basepkg}.filter.impl.ExtensionURLFilter")
  #set($urlRegex = "${basepkg}.filter.impl.RegexURLFilter")
  #set($robotsTxt = "${basepkg}.robot.impl.DefaultRobotsTxtProvider")
  #set($robotsMeta = "${basepkg}.robot.impl.DefaultRobotsMetaProvider")
  #set($headersRegex = "${basepkg}.filter.impl.RegexHeaderFilter")
  #set($docFetcher = "${basepkg}.fetch.impl.DefaultDocumentFetcher")
  #set($urlExtractor = "${basepkg}.url.impl.DefaultURLExtractor")
  #set($sitemap = "${basepkg}.sitemap.impl.DefaultSitemapResolver")
  #set($headersChecksummer = "${basepkg}.checksum.impl.DefaultHttpHeadersChecksummer")
  #set($docChecksummer = "${basepkg}.checksum.impl.DefaultHttpDocumentChecksummer")
  
  <!-- Location where internal progress files are stored. -->
  <progressDir>$workdir/progress</progressDir>

  <!-- Location where logs are stored. -->
  <logsDir>$workdir/logs</logsDir>

  <!-- All crawler configuration options can be specified as default 
       (including start URLs).  Settings defined here will be inherited by 
       all individual crawlers defined further down, unless overwritten.
       -->
  <crawlerDefaults>

    <!-- === Step 0: General Configuration ================================  -->
    
    <!-- Mandatory starting URL(s) where crawling begins.  If you put more 
         than one URL, they will be processed together.  If you are interested
         in providing a seed list instead, feel free to  -->    
    <startURLs>
      <url>http://www.google.com</url>
      <url>http://www.khwinana.co.za</url>
    </startURLs>

    <!-- Optional URL normalization feature. The class must implement
         *.url.IURLNormalizer, like the following class does.
      -->
    <urlNormalizer class="com.norconex.collector.http.url.impl.GenericURLNormalizer">
      <normalizations>
        lowerCaseSchemeHost, upperCaseEscapeSequence, decodeUnreservedCharacters, removeDefaultPort 
      </normalizations>
      <replacements>
        <replace>
           <match>&amp;view=print</match>
           <replacement>&amp;view=html</replacement>
        </replace>
      </replacements>
    </urlNormalizer>

    <!-- Optional delay resolver defining how polite or aggressive you want
         your crawling to be.  The class must implement 
         *.delay.IDelayResolver.
         The following is the default implementation:
      -->
    <delay default="1000" ignoreRobotsCrawlDelay="true"
           class="com.norconex.collector.http.delay.impl.DefaultDelayResolver">
          <schedule dayOfWeek="from Monday to Friday" 
                    time="from 8:00 to 6:30">10000</schedule>
    </delay>

    <!-- How many threads you want a crawler to use.  Regardless of how many
         thread you have running, the frequency of each URL being invoked
         will remain dictated by the &lt;delay/&gt option above.  Using more
         than one thread is a good idea to ensure the delay is respected
         in case you run into single downloads taking more time than the
         delay specified. Default is 2 threads.
      -->
    <numThreads>2</numThreads>

    <!-- How many level deep can the crawler go. I.e, within how many clicks 
         away from the main page (start URL) each page can be to be considered.
         Beyond the depth specified, pages are rejected.
         The starting URLs all have a zero-depth.  Default is -1 (unlimited)
         -->
    <maxDepth>5</maxDepth>
    
    <!-- Stop crawling after how many successfully processed URLs.  
         A successful URL is one that is either new or modified, that was 
         not rejected, not deleted, or did not generate any error.  As an
         example, this is a URL that will end up in your search engine. 
         Default is -1 (unlimited)
          -->
    <maxURLs>-1</maxURLs>

    <!-- Crawler "work" directory.  This is where files dowloaded or created as
         part of crawling activities (besides logs and progress) get stored.
         It should be unique to each crawlers.
         -->
    <workDir>$workdir/crawledFiles</workDir>

    <!-- Keep downloaded files. Default is false (deletes them after they have
         been processed).
         -->
    <keepDownloads>false</keepDownloads>

    <!-- Whether to mark orphan pages for deletion.  Orphans are URLs/pages, 
         which on subsequent crawls can no longer be reached when spidering
         the site (there are no links pointing to that page anymore).
         Default behavior is false to keep them as they still exist.
         -->
    <deleteOrphans>false</deleteOrphans>

    <!-- One or more optional listeners to be notified on various crawling
         events (e.g. document rejected, document imported, etc). 
         Class must implement *.crawler.IHttpCrawlerEventListener
         -->
    <crawlerListeners>
      <listener class="za.co.moromasystems.segoko.SekgokoHttpCrawlerEventListener"/>
    </crawlerListeners>

    <!-- Factory class creating a database for storing crawl status and
         other information.  Classes must implement 
         *.db.ICrawlURLDatabaseFactory.  Default implementation is the 
         following.
         -->
    <crawlURLDatabaseFactory 
         class="com.norconex.collector.http.db.impl.DefaultCrawlURLDatabaseFactory" />

    <!-- === Step 1: Initialize HTTP Client ================================ -->

    <!-- Initialize the HTTP client use to make connections.  Classes
         must implement *.client.IHttpClientInitializer.
         Default implementation offers many options. The following shows
         a sample use of the default with credentials.
         -->
    <httpClientInitializer class="com.norconex.collector.http.client.impl.DefaultHttpClientInitializer">
      <!-- These apply to any authentication mechanism -->
      <authUsername>myusername</authUsername>
      <authPassword>mypassword</authPassword>
      <!-- These apply to FORM authentication -->
      <authUsernameField>field_username</authUsernameField>
      <authPasswordField>field_password</authPasswordField>
      <authURL>https://www.google.com/login.php</authURL>
      <!-- These apply to both BASIC and DIGEST authentication -->
      <authHostname>www.google.com</authHostname>
      <authPort>80</authPort>
      <authRealm>PRIVATE</authRealm>
    </httpClientInitializer>
    
    <!-- === Step 2 (Loop): Process a URL/Document ========================= -->

      <!-- Optionally filter URL BEFORE any download. Classes must
           implement *.filter.IURLFilter, like the following examples.
           -->
      <httpURLFilters>
        <filter class="$extension" onMatch="exclude" >
          jpg,gif,png,ico,css,js</filter>
        <filter class="$urlRegex">https://www.google.com/.*</filter>
      </httpURLFilters>

      <!-- Filter BEFORE download with RobotsTxt rules. Classes must
           implement *.robot.IRobotsTxtProvider.  Default implementation
           is the following.
           -->
      <robotsTxt ignore="false" class="$robotsTxt"/>
      
      <!-- Loads sitemap.xml URLs and adds adds them to URLs to process.
           Default implementation is the following.
           -->
      <sitemap ignore="false" lenient="false" class="$sitemap">
         <location>http://www.google.com/sitemap.xml</location>
      </sitemap>
      
      <!-- Fetch Document HTTP Headers.  Classes must implement
           *.fetch.IHttpHeadersFetcher.  The following is a simple
            implementation.
           -->
      <httpHeadersFetcher 
          class="com.norconex.collector.http.fetch.impl.SimpleHttpHeadersFetcher"
          validStatusCodes="200" />

      <!-- Optionally filter AFTER download of HTTP headers.  Classes must 
           implement *.fetch.IHttpHeadersFetcher.  
           -->
      <httpHeadersFilters>
        <filter class="$headersRegex" 
                onMatch="exclude"
                caseSensitive="false"
                header="Content-Type">.*css.*</filter>
      </httpHeadersFilters>        
        
      <!-- Generates a checksum value from document headers to find out if 
           a document has changed. Class must implement
           *.checksum.IHttpHeadersChecksummer.  Default implementation 
           is the following. 
           -->
      <httpHeadersChecksummer class="$headersChecksummer" />

      <!-- Fetches document.  Class must implement 
           *.fetch.IHttpDocumentFetcher.  Default implementation is the 
           following.
           -->
      <httpDocumentFetcher class="$docFetcher" validStatusCodes="200" />

      <!-- Establish whether to follow a page URLs or to index a given page
           based on in-page meta tag robot information. Classes must
           implement *.robot.IRobotsMetaProvider.  Default implementation
           is the following.
           -->
      <robotsMeta ignore="false" class="$robotsMeta" />

      <!-- Extract URLs from a document.  Classes must implement
           *.url.IURLExtractor. Default implementation is the following.
           -->
      <urlExtractor class="$urlExtractor" />

      <!--  Optionally filters a document. Classes must implement 
            *.filter.IHttpDocumentFilter-->
      <httpDocumentFilters>
          <filter class="com.norconex.collector.http.filter.impl.ExtensionURLFilter" />
          <filter class="com.norconex.collector.http.filter.impl.RegexURLFilter" />
      </httpDocumentFilters>

      <!-- Optionally process a document BEFORE importing it. Classes must
           implement *.doc.IHttpDocumentProcessor.
           -->
      <preImportProcessors>
         <processor class="za.co.moromasystems.segoko.SegokoHttpDocumentProcessor"></processor>
      </preImportProcessors>
        
      <!-- Import a document.  This step calls the Importer module.  The
           importer is a different module with its own set of XML configuration
           options.  Please refer to importer for complete documentation.
           Below gives you an overview of the main importer tags.
           -->
      <importer>
          <preParseHandlers>
              <tagger class="com.norconex.importer.tagger.impl.RenameTagger"/>
              <transformer class="com.norconex.importer.transformer.impl.StripBeforeTransformer" />
              <filter class="com.norconex.importer.filter.impl.RegexMetadataFilter" />
          </preParseHandlers>
          <documentParserFactory class="com.norconex.importer.parser.DefaultDocumentParserFactory" />
          <postParseHandlers>
              <tagger class="com.norconex.importer.tagger.impl.ReplaceTagger"/>
              <transformer class="com.norconex.importer.transformer.impl.StripAfterTransformer" />
              <filter class="com.norconex.importer.filter.impl.RegexMetadataFilter" />
          </postParseHandlers>
      </importer>           

      <!-- Create a checksum out of a document to figure out if a document
           has changed, AFTER it has been imported.
           Class must implement *.checksum.IHttpDocumentChecksummer.
           Default implementation is the following.
           -->
      <httpDocumentChecksummer class="$docChecksummer" />

      <!-- Optionally process a document AFTER importing it. Classes must
           implement *.doc.IHttpDocumentProcessor.
           -->
      <postImportProcessors>
         <processor class="za.co.moromasystems.segoko.SegokoPostHttpDocumentProcessor"></processor>
      </postImportProcessors>
        
      <!-- Commits a document to a data source of your choice.
           This step calls the Committer module.  The
           committer is a different module with its own set of XML configuration
           options.  Please refer to committer for complete documentation.
           Below is an example using the FileSystemCommitter.
           -->
      <committer class="com.norconex.committer.impl.FileSystemCommitter">
        <directory>$workdir\crawledFiles</directory>
      </committer>

  </crawlerDefaults>


  <!-- Individual crawlers can be defined here.  All crawler default
       configuration settings will apply to all crawlers created unless 
       explicitly overwritten in crawler configuration.
       For configuration options where multiple items can be present 
       (e.g. filters), the whole list will in crawler defaults would be
       overwritten.
       Since the options are the same as the defaults above, the documentation 
       is not repeated here.
       The only difference from "crawlerDefaults" is the addition of the "id"
       attribute on the crawler tag.  The "id" attribute uniquelly identifies
       each of your crawlers.  
       -->
  <crawlers>
    <crawler id="TestWebSite">
       <!-- Overwrite any defaults here. -->
    </crawler>
  </crawlers>

</httpcollector>